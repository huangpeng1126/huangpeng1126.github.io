<!DOCTYPE html>
<html lang="zh-CN">
    <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
    <meta charset="UTF-8" />

    <meta name="generator" content="Hugo 0.104.3" /><meta name="theme-color" content="#fff" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <meta name="format-detection" content="telephone=no, date=no, address=no, email=no" />
    
    <meta http-equiv="Cache-Control" content="no-transform" />
    
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <title>Xgboost_lightgbm_catboost分析 | 瞌睡鱼&amp;花脸猫 个人网站</title>

    <link rel="stylesheet" href="/css/meme.min.0bf79c6291c64b98be75f8b1a99bca305a0cc8999a7b59896522fd731aebdf7e.css"/>

    
    
        <script src="/js/meme.min.1343c21422ddd723ef2bde0b482ea241020c7d5c128b72528dd13d375565510b.js"></script>

    

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />

        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" media="print" onload="this.media='all'" />
        <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=EB&#43;Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto&#43;Serif&#43;SC:wght@400;500;700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" /></noscript>

    <meta name="author" content="瞌睡鱼" /><meta name="description" content="xgboost 模型定义 什么是XGBoost：[官方文档](Introduction to Boosted Trees — xgboost 2.0.0-dev documentation)……" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2a6df4" />
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-title" content="瞌睡鱼&amp;花脸猫 个人网站" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black" />
    <meta name="mobile-web-app-capable" content="yes" />
    <meta name="application-name" content="瞌睡鱼&amp;花脸猫 个人网站" />
    <meta name="msapplication-starturl" content="../../" />
    <meta name="msapplication-TileColor" content="#fff" />
    <meta name="msapplication-TileImage" content="../../icons/mstile-150x150.png" />
    <link rel="manifest" href="/manifest.json" />

    
    

    
    <link rel="canonical" href="https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/" />
    

<script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "datePublished": "2022-06-28T11:33:19+08:00",
        "dateModified": "2022-08-19T14:59:48+08:00",
        "url": "https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/",
        "headline": "Xgboost_lightgbm_catboost分析",
        "description": "xgboost 模型定义 什么是XGBoost：[官方文档](Introduction to Boosted Trees — xgboost 2.0.0-dev documentation)……",
        "inLanguage" : "zh-CN",
        "articleSection": "posts",
        "wordCount":  7984 ,
        "image": ["https://pic2.zhimg.com/80/v2-c108a582cd03dae298c5c51305498cf5_1440w.jpg","https://pic1.zhimg.com/v2-e0ab9287990a6098e4cdbc5a8cff4150_r.jpg","https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/struct_score.png","https://machinelearningmastery.com/wp-content/uploads/2016/02/Example-Decision-Tree.png","https://pic4.zhimg.com/80/v2-5f16246289eaa2a3ae72f971db198457_1440w.jpg","https://www.researchgate.net/publication/346577317/figure/fig3/AS:1001743705993216@1615845719667/Histogram-algorithm.png","https://www.researchgate.net/publication/350848994/figure/fig2/AS:1019742676582401@1620137008154/Histogram-algorithm-of-LightGBM.ppm","https://pic4.zhimg.com/v2-3064f201bc8545f851c7ccf47921c0e7_r.jpg","https://pic4.zhimg.com/v2-b51f2764c13ca0a7b4cb41849a367a87_b.jpg"],
        "author": {
            "@type": "Person",
            "description": "心有猛虎细嗅玫瑰",
            "email": "huangpeng1126@126.com",
            "image": "https://huangpeng1126.github.io/icons/apple-touch-icon.png",
            "url": "https://huangpeng1126.github.com/",
            "name": "瞌睡鱼"
        },
        "license": "[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)",
        "publisher": {
            "@type": "Organization",
            "name": "瞌睡鱼\u0026花脸猫 个人网站",
            "logo": {
                "@type": "ImageObject",
                "url": "https://huangpeng1126.github.io/icons/apple-touch-icon.png"
            },
            "url": "https://huangpeng1126.github.io/"
        },
        "mainEntityOfPage": {
            "@type": "WebSite",
            "@id": "https://huangpeng1126.github.io/"
        }
    }
</script>

    

<meta name="twitter:card" content="summary_large_image" />


<meta name="twitter:site" content="@reuixiy" />
<meta name="twitter:creator" content="@huangpeng1126" />

    



<meta property="og:title" content="Xgboost_lightgbm_catboost分析" />
<meta property="og:description" content="xgboost 模型定义 什么是XGBoost：[官方文档](Introduction to Boosted Trees — xgboost 2.0.0-dev documentation)……" />
<meta property="og:url" content="https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/" />
<meta property="og:site_name" content="瞌睡鱼&amp;花脸猫 个人网站" />
<meta property="og:locale" content="zh" /><meta property="og:image" content="https://pic2.zhimg.com/80/v2-c108a582cd03dae298c5c51305498cf5_1440w.jpg" />
<meta property="og:type" content="article" />
    <meta property="article:published_time" content="2022-06-28T11:33:19&#43;08:00" />
    <meta property="article:modified_time" content="2022-08-19T14:59:48&#43;08:00" />
    
    <meta property="article:section" content="posts" />



    
    

    
</head>

    <body>
        <div class="container">
            
    <header class="header">
        
            <div class="header-wrapper">
                <div class="header-inner single">
                    
    <div class="site-brand">
        
            <a href="/" class="brand">瞌睡鱼&amp;花脸猫 个人网站</a>
        
    </div>

                    <nav class="nav">
    <ul class="menu" id="menu">
        
            
        
        
        
        
            
                <li class="menu-item"><a href="/posts/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon archive"><path d="M32 448c0 17.7 14.3 32 32 32h384c17.7 0 32-14.3 32-32V160H32v288zm160-212c0-6.6 5.4-12 12-12h104c6.6 0 12 5.4 12 12v8c0 6.6-5.4 12-12 12H204c-6.6 0-12-5.4-12-12v-8zM480 32H32C14.3 32 0 46.3 0 64v48c0 8.8 7.2 16 16 16h480c8.8 0 16-7.2 16-16V64c0-17.7-14.3-32-32-32z"/></svg><span class="menu-item-name">文章</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/categories/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon th"><path d="M149.333 56v80c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V56c0-13.255 10.745-24 24-24h101.333c13.255 0 24 10.745 24 24zm181.334 240v-80c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256 0 24.001-10.745 24.001-24zm32-240v80c0 13.255 10.745 24 24 24H488c13.255 0 24-10.745 24-24V56c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24zm-32 80V56c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256 0 24.001-10.745 24.001-24zm-205.334 56H24c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24zM0 376v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H24c-13.255 0-24 10.745-24 24zm386.667-56H488c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zm0 160H488c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zM181.333 376v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24z"/></svg><span class="menu-item-name">分类</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/tags/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" class="icon tags"><path d="M497.941 225.941L286.059 14.059A48 48 0 0 0 252.118 0H48C21.49 0 0 21.49 0 48v204.118a48 48 0 0 0 14.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882 0l204.118-204.118c18.745-18.745 18.745-49.137 0-67.882zM112 160c-26.51 0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882 0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397 0h48.721a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882z"/></svg><span class="menu-item-name">标签</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/about/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon user-circle"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"/></svg><span class="menu-item-name">关于</span></a>
                </li>
            
        
            
                
                    
                    
                        <li class="menu-item">
                            <a id="theme-switcher" href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5l48.8-97.5a18 18 0 0128 0l48.8 97.5 103.4 -34.5a18 18 0 0119.8 19.8l-34.5 103.4l97.5 48.8a18 18 0 010 28l-97.5 48.8 34.5 103.4a18 18 0 01-19.8 19.8l-103.4-34.5-48.8 97.5a18 18 0 01-28 0l-48.8-97.5l-103.4 34.5a18 18 0 01-19.8-19.8l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8-34.5-103.4a18 18 0 0119.8-19.8zM256 128a128 128 0 10.01 0M256 160a96 96 0 10.01 0"/></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412a256 256 0 10154-407a11.5 11.5 0 00-5 20a201.5 201.5 0 01-134 374a11.5 11.5 0 00-15 13"/></svg></a>
                        </li>
                    
                
            
        
            
                
            
        
    </ul>
</nav>

                    
                </div>
            </div>
            
    <input type="checkbox" id="nav-toggle" aria-hidden="true" />
    <label for="nav-toggle" class="nav-toggle"></label>
    <label for="nav-toggle" class="nav-curtain"></label>


        
    </header>




            
            
    <main class="main single" id="main">
    <div class="main-inner">

        

        <article class="content post h-entry" data-align="justify" data-type="posts" data-toc-num="true">

            <h1 class="post-title p-name">Xgboost_lightgbm_catboost分析</h1>

            

            
                
            

            
                

<div class="post-meta">
    
        
        <time datetime="2022-06-28T11:33:19&#43;08:00" class="post-meta-item published dt-published"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M148 288h-40c-6.6 0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h48c26.5 0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3 0 6-2.7 6-6z"/></svg>&nbsp;2022.6.28</time>
    
    
        
        <time datetime="2022-08-19T14:59:48&#43;08:00" class="post-meta-item modified dt-updated"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M400 64h-48V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H160V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H48C21.49 64 0 85.49 0 112v352c0 26.51 21.49 48 48 48h352c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm-6 400H54a6 6 0 0 1-6-6V160h352v298a6 6 0 0 1-6 6zm-52.849-200.65L198.842 404.519c-4.705 4.667-12.303 4.637-16.971-.068l-75.091-75.699c-4.667-4.705-4.637-12.303.068-16.971l22.719-22.536c4.705-4.667 12.303-4.637 16.97.069l44.104 44.461 111.072-110.181c4.705-4.667 12.303-4.637 16.971.068l22.536 22.718c4.667 4.705 4.636 12.303-.069 16.97z"/></svg>&nbsp;2022.8.19</time>
    
    
    
        
        
        
            
        
    
    
        
        <span class="post-meta-item wordcount"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;7984</span>
    
    
        
        <span class="post-meta-item reading-time"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5 0-200-89.5-200-200S145.5 56 256 56s200 89.5 200 200-89.5 200-200 200zm61.8-104.4l-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6 0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;16&nbsp;分钟</span>
    
    
    
</div>

            

            <div class="post-body e-content">
                <h2 id="xgboost"><a href="#xgboost" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>xgboost</h2>
<h3 id="模型定义"><a href="#模型定义" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>模型定义</h3>
<blockquote>
<p>什么是XGBoost：[官方文档](<a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html#" target="_blank" rel="noopener">Introduction to Boosted Trees — xgboost 2.0.0-dev documentation</a>)</p>
<p>XGBoost 是基于决策树的集成机器学习算法，它以梯度提升（Gradient Boost）为框架。在非结构数据（图像、文本等）的预测问题中，人工神经网络的表现要优于其他算法或框架。但在处理中小型结构数据或表格数据时，现在普遍认为基于决策树的算法是最好的。下图列出了近年来基于树的算法的演变过程：</p>
<p><img src="https://pic2.zhimg.com/80/v2-c108a582cd03dae298c5c51305498cf5_1440w.jpg" alt=""></p>
<p>XGBoost 算法最初是华盛顿大学的一个研究项目。<a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650760514%26idx%3D2%26sn%3Df9e03c2e4aead6098e30111493f49d28%26chksm%3D871aa13cb06d282a227d056740803c69c4c978d5bdb87fc1b30f1d1ed57c0c01cf8ec99657dd%26token%3D1906738629%26lang%3Dzh_CN" target="_blank" rel="noopener">陈天奇</a>和 Carlos Guestrin 在 SIGKDD 2016 大会上发表的论文《XGBoost: A Scalable Tree Boosting System》在整个机器学习领域引起轰动。自发表以来，该算法不仅多次赢得 Kaggle 竞赛，还应用在多个前沿工业应用中，并推动其发展。许多数据科学家合作参与了 XGBoost 开源项目，GitHub 上的这一项目（<a href="https://link.zhihu.com/?target=https%3A//github.com/dmlc/xgboost/" target="_blank" rel="noopener">https://github.com/dmlc/xgboost/</a>）约有 350 个贡献者，以及 3600 多条提交。和其他算法相比，XGBoost 算法的不同之处有以下几点：</p>
<ul>
<li>
<p>应用范围广泛：该算法可以解决回归、分类、排序以及用户自定义的预测问题</p>
</li>
<li>
<p>语言：支持包括 C++、Python、R、Java、Scala 和 Julia 在内的几乎所有主流编程语言</p>
</li>
<li>
<p>云集成：支持 AWS、Azure 和 Yarn 集群，也可以很好地配合 Flink、 Spark 等其他生态系统</p>
</li>
</ul>
</blockquote>
<p>xgboost目标函数：我们知道XGBoost是由<code>k</code>个基模型组成的一个加法运算：</p>
<p>$$
\hat{y_i}^{(t)} = \sum_{k=1}^t f_k(x_i) = \hat{y_i}^{(t-1)} + f_t(x_i)
$$</p>
<p>xgboost的模型基本上可以用上面的公式来表达：</p>
<ul>
<li>
<p>xgboost是一个加法模型，由总共<code>k</code>颗数模型来表达。具体表现为训练过程中一直向前迭代，首先训练第一棵树，假设100分，达到了80分（实际的代码实现过程中一般都会降权，例如降权因子为0.1，实际得分就为8分）。然后依据第一棵树的表现开始训练第二棵树（在第一棵树结果的基础上开始训练）</p>
</li>
<li>
<p>NOTES：$\hat{y}^{(t)}$是第<code>t</code>次迭代后样本<code>i</code>的预测结果；$f_t(x_i)$是第<code>t</code>棵树的模型预测结果；$\hat{y}^{(t-1)}$是第<code>t-1</code>棵树的预测结果</p>
</li>
</ul>
<h3 id="目标函数---损失函数"><a href="#目标函数---损失函数" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>目标函数 - 损失函数</h3>
<p>$$
Obj = \sum_{i=1}^n l(y_i, \hat{y}<em>i) + \sum</em>{j=1}^t \Omega(f_j)
$$</p>
<p>其中$l(y_i,\hat{y}<em>i)$是我们模型的损失函数，$\hat{y}^i$是整个模型在第<code>i</code>个样本上的预测值，$y_i$是第<code>i</code>个样本的真实值。NOTES：$\sum</em>{j=1}^t \Omega(f_j)$是全部<code>t</code>棵树的复杂度求和，在实际实现过程中一般用<code>L1</code>或<code>L2</code>正则表达式</p>
<p>从这个目标函数我们需要掌握的细节：① 模型是加性函数，向前迭代；② 前后部分是两个维度的计算问题。</p>
<p><strong>两个累加的变量是不同的</strong></p>
<ul>
<li>
<p>一个是<code>i</code>，前面分式代表的是训练样本数量，也就是对每个样本我们都会计算一个损失，这个损失是总共<code>t</code>棵树的预测值之和与样本真实值之间的差值计算</p>
</li>
<li>
<p>另一个是累加变量<code>j</code>，代表的是<strong>树</strong>的数量，也就是我们对每棵树的复杂度进行累加计算</p>
</li>
</ul>
<p>我们继续往前推导，$\hat{y}_i^t = \hat{y}_i^{(t-1)} + f_t(x_i)$，其中$\hat{y}_i^{(t-1)}$是第<code>t-1</code>步的模型给出的预测值，是已知常数，$f_t(x_i)$使我们当前需要加入（训练）的新模型的预测值，此时目标函数可以进一步表示为：</p>
<p>$$
\begin{equation*}
\begin{split}
Obj^t &amp;= \sum_{i=1}^n l(y_i, \hat{y}<em>i^t) + \sum</em>{i=1}^t \Omega(f_i) \
&amp;= \sum_{i=1}^n l(y_i, \hat{y}_i^{t-1} + f_t(x_i)) + \sum_i^t \Omega(f_i)
\end{split}
\end{equation*}
$$</p>
<p>此时要优化目标函数，等价于要求解$f_t(x_i)$。</p>
<blockquote>
<p>泰勒公式是将一个在$x=x_0$楚具有<code>n</code>阶导数的函数$f(x)$利用关于$x-x_0$ 的 <code>n</code>次多项式来逼近函数的方法，若函数$f(x)$在包含$x_0$的某个闭区间<code>[a,b]</code>上具有 <code>n</code> 阶导数，且在开区间$(a,b)$ 上具有<code>n+1</code>阶导数，则对闭区间 <code>[a,b]</code>上任意一点$x$有$f(x)=\sum_{i=0}^n \frac{f^{(i)}}{i!}(x-x_0)^i + R_n(x)$，其中多项式称为函数在$x_0$处的泰勒展开式，$R_n(x)$是泰勒公式的余项并且是$(x-x_0)^n$的高阶无穷小。</p>
</blockquote>
<p>根据泰勒公式我们可以把函数$f(x+\Delta{x})$在点$x$处进行泰勒的二阶展开，可以得到下面的等式：</p>
<p>$$
f(x+\Delta{x}) \approx f(x) + f^{'}(x)\Delta{x} + \frac{1}{2}f^{''}\Delta{x}^2
$$</p>
<blockquote>
<p><strong>关注点：</strong></p>
<p>上面的公式中，$\Delta{x}$对应的是第<code>t</code>颗树的模型$f_t(x)$，<strong>$\Delta{x}$对应的是一颗树模型，是一个树模型，是一个树模型，重要的事情讲三遍，是一颗树模型，而不是一个具体的标量数值。</strong> 而$x$对应的是$\hat{y}^{t-1}$，所以最后相应的损失函数应该是$l(y_i,\hat{y}_i^{(t-1)}+f_t(x_i)$</p>
</blockquote>
<p>我们把$\hat{y}^{t-1}$看做是$x$，$f_t(x_i)$看做是$\Delta{x}$，因此可以将目标函数写为：</p>
<p>$$
Obj^{(t)} = \sum _{i=1} ^n \left [ l(y_i, \hat{y}<em>i^{t-1}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right ] + \sum</em>{i=1}^t \Omega(f_i)
$$</p>
<p>其中$g_i$为损失函数的一阶求导，$h_i$为损失函数的二阶求导，<strong>注意这里的被求导函数是$\hat{y}_i^{t-1}$</strong>。</p>
<p>$$
\begin{equation*}
\begin{split}
g_i &amp;= \partial_{\hat{y}^{t-1}} l(y_i, \hat y^{t-1}) \
h_i &amp;= \partial^2_{\hat{y}^{t-1}} l(y_i, \hat y^{t-1}) \
\end{split}
\end{equation*}
$$</p>
<p>我们以平方损失函数举例：</p>
<p>$$
\sum_{i=1}^n(y_i - (\hat{y}_i^{t-1}+f_t(x_i)))^2
$$</p>
<p>则可以写出：</p>
<p>$$
\begin{equation*}
\begin{split}
g_i &amp;= \partial_{\hat{y}^{t-1}} (\hat{y}^{t-1} - y_i)^2 = 2(\hat{y}^{t-1} - y_i)  \
h_i &amp;= \partial^2_{\hat{y}^{t-1}}  (\hat{y}^{t-1} - y_i)^2  = 2\
\end{split}
\end{equation*}
$$</p>
<p>由于在第<code>t</code>步时$\hat{y}^{t-1}$已经是一个已知的值所以$l(y_i, \hat{y}_i^{t-1})$是一个常数，其对函数的优化不会影响最终的结果，所以目标函数可以简写成：</p>
<p>$$
Obj^{(t)} \approx \sum <em>{i=1} ^n \left [ g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right ] + \sum</em>{i=1}^t \Omega(f_i)
$$</p>
<p>所以我们只需要求出每一步损失函数的一阶导和二阶导的值（由于前一步的$\hat{y}^{t-1}$是已知的，所以这两个值是常数），然后最优化目标函数，就可以得到每一步的$f(x)$，最后根据加法模型得到一个整体模型。</p>
<h3 id="目标函数---正则化"><a href="#目标函数---正则化" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>目标函数 - 正则化</h3>
<p>机器学习中，训练过程中降低损失函数是为了提升训练精度，具体就是使得我们的模型经过训练和训练样本的拟合度更高，从机器学习角度描述是为了降低模型的<code>偏差</code>(bias)。但是为了模型效果在实际业务上表现能够和训练时保持一致的高精度，我们还需要通过其它方式来达成，这个过程被称为降低模型的<code>方差</code>(variance)，而最常用的技术方案是通过正则化来实现(regulization)。通俗的讲正则化是通过降低模型的复杂度来实现降低模型方差。为了后续的描述和推导方便，我们首先引入一些符号定义：</p>
<p>$f_t(x) = w_{q(x)}$，$x$为某一个样本，等式中$q(x)$代表该样本在哪个叶子结点，而$w_q$则代表了样本所在叶子结点的取值$w$，所以$w_{q(x)}$就代表了每个样本$x$的具体取值（即预测值）。</p>
<p>xgboost的基模型使用的是分类回归决策树（CART），而树模型的复杂度可以由叶子结点$T$表示，基于一个朴素的观点：叶子少的树比叶子多的树更加简单，也就表示叶子结点少的树模型在位置数据上的表现更加稳定（这是一个很朴素但是正确的结论，现在基本上所有类型的模型在正则化方面都是基于这个朴素假设），假设叶子节点总数为$T$；另外，更加稳定的树模型，它的叶子结点的权重$w$也不能过高，所以目标函数的正则化计算项可以重写成：</p>
<p>$$
\Omega{(f_t)} = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_i^2
$$</p>
<p>即决策树模型最终的复杂度有生产的所有决策树的叶子节点数量和所有叶子节点权重所组成的向量的$L_2$范式共同决定，如下图所示：</p>
<p><img src="https://pic1.zhimg.com/v2-e0ab9287990a6098e4cdbc5a8cff4150_r.jpg" alt=""></p>
<p>上图描述了基于决策树的XGBoost的正则式计算方式。</p>
<p>我们设$I_j = {i | q_{x_i}=j}$为决策树<code>q</code>的第<code>j</code>个叶子节点的样本集合，然后可以重写目标函数为：</p>
<p>$$
\begin{equation*}
\begin{split}
Obj^{(t)} &amp;\approx \sum <em>{i=1} ^n \left [ g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right ] + \sum</em>{i=1}^t \Omega(f_i) \
&amp;= \sum_{i=1}^n \left[g_i w_{q_{(x_i)}} + \frac{1}{2} h_i  w^2_{q_{(x_i)}}   \right] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2 \
&amp;= \sum_{j=1}^T \left[(\sum_{i \in I_j} g_i) w_j + \frac{1}{2} (\lambda + \sum_{i \in I_j}h_i) w_j^2 \right] + \gamma T \
&amp;, w_j \in R^T(叶子节点权重空间) \
&amp;, q: R^d -&gt; {1,2,\cdots,T}（树结构，将样本映射到叶子节点）
\end{split}
\end{equation*}
$$</p>
<blockquote>
<p>NOTES：第二步到第三步的推导不太直观，可以从概念上去理解会更加容易。第二步的操作是先遍历所有样本，然后计算每个样本的损失函数，但是注意观察，损失函数的计算最终都是在<strong>有效</strong>的叶子节点上。这里的<code>有效</code>是指针对一个具体样本，它最终是落在每棵决策树的具体一个节点上，最终生效的叶子节点其实是模型中决策树的数量。这里遍历所有的节点数量<code>T</code>，其实针对具体一个样本很多叶子节点是无效的，但是针对样本集基本上每个叶子节点都存在若干样本。所以，三步的逻辑是我们从遍历样本转换成先遍历叶子节点，然后遍历每个叶子节点上的样本，实现了步骤二相同的计算逻辑</p>
</blockquote>
<p>为了简化公式，我们定义：$G_j = \sum_{i \in I_j} g_i$，$H_j = \sum_{i \in I_j} h_i$，则目标函数简化为：</p>
<p>$$
Obj^{(t)} = \sum_{j=1}^T \left[ G_j w_j + \frac{1}{2} (\lambda + H_j)w_j^2 \right] + \gamma T
$$</p>
<p>进一步观察，我们可以看到$G_j, H_j$都是前面<code>t-1</code>步骤已经计算出来的，所以只有最后一棵树的叶子节点$w_j$是未知数，我们要计算目标函数的最小值。因此，针对固定的决策树结构（即固定结构的$q$）求解$w_j$一阶求导，并令其为0，则可以求解出叶子节点<code>j</code>对应的权值：</p>
<p>$$
w^{\star}<em>j = - \frac {\sum</em>{i \in I_j}g_i} {\sum_{i \in I_j}h_i  + \lambda} = - \frac {G_j} {H_j+\lambda}
$$</p>
<p>所以，目标函数可以简化为：</p>
<p>$$
Obj^{t} = -\frac{1}{2} \times \sum\limits_{j=1}^T\frac{ G_j^2}{H_j+\lambda} + \gamma T
$$</p>
<p>下面给出一个具体的图示来展示计算：</p>
<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/struct_score.png" alt=""></p>
<blockquote>
<p>计算过程：</p>
<p>① 计算每个样本的一阶导数和二阶导数
② 针对每个节点所包含的样本求和得到$G_j$和$H_j$</p>
<p>③ 遍历决策树节点可以得到目标函数</p>
</blockquote>
<p>到目前为止，我们主要在介绍如何通过样本来更新模型的权重（即叶子节点的权重值）。但是XGBoost的模型不仅仅是叶子节点权重，还包括<code>Tree Structure</code>。是的，XGBoost的模型包括：<code>Tree Structure + Leaf Weight</code>两部分组成。前文一直在介绍如何更新叶子权重，下面将介绍前面一部分，即如何构建CART树。</p>
<h3 id="cart树构建"><a href="#cart树构建" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>CART树构建</h3>
<blockquote>
<p>Classification and Regression Tree (CART) for short is a term introduced by <a href="https://en.wikipedia.org/wiki/Leo_Breiman" target="_blank" rel="noopener">Leo Breiman</a> to refer to <a href="https://en.wikipedia.org/wiki/Decision_tree_learning" target="_blank" rel="noopener">Decision Tree</a> algorithms that can be used for classification or regression predictive modeling problems. Classically, this algorithm is referred to as “decision trees”, but on some platforms like R they are referred to by the more modern term CART. The CART algorithm provides a foundation for important algorithms like bagged decision trees, random forest and boosted decision trees.</p>
</blockquote>
<p>In a word，CART就是决策树的说法，它通常是一颗二叉树，每一个节点代表了对于一个特征维度的二维划分，而叶子节点包含了一个具体的数值<code>y</code>，被作为一个输入样本的预测值。</p>
<p>假设我们有一个数据集只有两个特征：高度（单位为厘米）、长度（单位是千米），希望通过这两个维度的数字来预测一个输入$x={(x_1,x_2)}$所属的性别<code>F / M</code>，下图是一个简单的示例：</p>
<p><img src="https://machinelearningmastery.com/wp-content/uploads/2016/02/Example-Decision-Tree.png" alt=""></p>
<p>从上面的描述可以看出，一颗决策树实际上是一个对输入空间的划分，我们可以把每一个输入的变量当做是一个<code>p维空间</code>里的一个维度，而决策树把空间划分成一个多维举矩阵的空间。</p>
<img title="" src="https://scikit-learn.org/stable/_images/iris.svg" alt="" width="498" data-align="center">
<p>常见决策树有 <code>ID3</code>、<code>C4.5/5.0</code>、<code>CART</code>，其中<code>ID3</code>和<code>C4.5</code>生成的决策树是多叉树，只能处理分类而不能处理回归。而CART是分类回归树，既可以分类也可以回归。</p>
<p>ID3使用<code>信息增益</code>做为特征选择器来分割节点，C4.5使用的是<code>信息增益率</code>选择特征（避免信息增益指标偏向于多值属性），CART树选择的是<code>Gini Index</code>基尼系数选择特征，基尼系数代表了模型的不纯度，<strong>基尼系数越小，不纯度越低，特征越好</strong>。这和信息增益（率）相反。</p>
<h4 id="gini-index"><a href="#gini-index" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Gini Index</h4>
<p>数据集<code>D</code>的纯度可以用基尼值来衡量：</p>
<p>$$
Gini(D) = \sum\limits_{i=1}^n p(x_i)(1-p(x_i)) = 1 - \sum_{i=1}^n p(x_i)^2
$$</p>
<p>其中，数据集<code>D</code>可以看成是CART树上具体一个节点上的数据集，通常是全部训练集的一个子集（可以简单看做是训练过程中输入到这个节点的数据集），$p(x_i)$是分类$x_i$出现的概率，<code>n</code>是分类的数量，Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，<strong>Gini(D)越小，则[数据集D的纯度越高</strong>。</p>
<p>对于样本D，个数为|D|，根据特征A 是否取某一可能值$\alpha$，把样本D分成两部分 $D_1,D_2$，所以CART树建立的是一颗二叉树：</p>
<p>$$
D_1=(x,y) \in D | A(x) = \alpha, D_2 = D - D_1
$$</p>
<p>在属性A的条件下，样本D的基尼系数定义为：</p>
<p>$$
GiniIndex(D | A \le \alpha) = \frac{|D_1|}{|D|} Gini(D_1) + \frac{|D_2|}{|D|} Gini(D_2)
$$</p>
<h4 id="最优切分点划分算法"><a href="#最优切分点划分算法" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>最优切分点划分算法</h4>
<h5 id="贪心算法"><a href="#贪心算法" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>贪心算法</h5>
<p>贪心算法在XGBoost里通过设置参数<code>tree_method=exact</code>来选择。</p>
<blockquote>
<ol>
<li>
<p>从深度为0的树开始，第一个叶子节点开始（包含所有样本），对每个特征计算最佳分裂值，并获取分裂收益。计算逻辑是按照<strong>每个特征首先按照特征值做升序排列</strong>，然后线性扫描的方式来计算并确定最佳分裂点，并记录该分裂的收益值（计算公式下面内容介绍）</p>
</li>
<li>
<p>步骤<code>1</code>中选取收益最大的分裂特征和分裂点进行分裂，生成两个子节点，并未新节点关联对应的样本集</p>
</li>
<li>
<p>回到步骤<code>1</code>，递归执行知道满足特定条件（最大树深度、最大子节点数量、无法达到分裂条件等）</p>
</li>
<li>
<p>将训练得到的CART树加入到模型中：$\hat{y}^{(t)} = \hat{y}^{(t-1)} + f_t(x)$</p>
<p>① 通常在实际应用中我们为了防止过拟合： $\hat{y}^{(t)} = \hat{y}^{(t-1)} + \eta f_t(x)$
② $\eta$通常被称为学习步长或学习率，通常设置为0.1</p>
<p>③ 这意味着我们在训练的时候是朝着最优化目标求解模型参数（即求解$f_t(x)$的时候不使用参数$\eta$），但是在预测的时候使用（同时意味着在训练$f_{t+1}(x)$的时候超参数$\eta$会发挥作用）</p>
</li>
</ol>
</blockquote>
<p>介绍分裂收益计算公式前，我们先引入一些符号定义。假设$I_L$和$I_R$是节点分裂后左节点和右节点的样本集，则分裂前的样本集合为$I=I_L \cup I_R$，那么分裂收益计算公式如下：</p>
<p>$$
\begin{equation*}
\begin{split}
Gain_{split} &amp;= \frac{1}{2} \left[\frac{(\sum_{i \in I_L} g_i)^2}{\sum_{i \in I_L}h_i + \lambda} +
\frac{(\sum_{i \in I_R} g_i)^2}{\sum_{i \in I_R}h_i + \lambda}
- \frac{(\sum_{i \in I} g_i)^2}{\sum_{i \in I}h_i + \lambda} \right]
- \gamma  \
&amp;= \frac{1}{2} \left[<br>
\frac{G_L^2}{H_L+\lambda}
+ \frac{G_R^2}{H_R+\lambda}
- \frac{(G_L+G_R)^2}{(H_L+H_R+\lambda}<br>
\right] - \gamma
\end{split}
\end{equation*}
$$</p>
<p>我们可以发现对于所有的分裂点$\alpha$，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和$G_L$和 $G_R$。然后用上面的公式计算每个分割方案的分数就可以了。</p>
<h4 id="近似算法"><a href="#近似算法" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>近似算法</h4>
<p>贪心算法在求解每个特征的最佳分裂点的时候会遍历这个特征所有的取值，并且在遍历前会排序，在数据量大的情况下是xgboost最耗时的地方。为了提升训练效率，xgboost内部实现了近似算法，通过设置参数<code>tree_method=approx</code></p>
<p>针对每个特征，<code>exact</code>算法会遍历每个特征值，十分耗时。近似算法会针对每个特征的取值范围划分<code>分位点</code>，每个分位点代表了不同特征值包含训练样本的数量占比。下面给出<code>appox</code>算法的伪代码</p>
<blockquote>
<p>// Approximate Algorithm for Split Finding in XGBoost
for k=1 to m do
    Propose $S_k = {s_{k1}, s_{k2}, ..., s_{km}}$ by percentiles on feature k
    Proposal can be done per tree (global), or per split (local)
end</p>
<p>for k=1 to m do
    $G_{kv} =  \sum_{j \in {j|s_{k,v} \ge x_{j,k} \ge s_{k,v-1} g_j} }$
$H_{kv} =  \sum_{j \in {j|s_{k,v} \ge x_{j,k} \ge s_{k,v-1} h_j} }$
end</p>
<p>Follow the same steps as in previous section to find max score only among proposed splits</p>
</blockquote>
<ul>
<li>
<p><strong>第一个<code>for</code>循环</strong>：对特征 k 根据该特征分布的分位数找到切割点的候选集合 $S_k = {s_{k1}, s_{k2}, ..., s_{km}}$</p>
</li>
<li>
<p><strong>针对第二个<code>for</code>循环</strong>：针对每个特征的候选集合，将样本映射到由该特征对应的候选点集构成的分桶区间中，即$s_{k,v} \ge x_{j,k} \ge s_{k,v-1}$，对每个桶统计$G,H$值，最后在这些统计量上寻找最佳分裂点</p>
</li>
</ul>
<p>下面是<code>approx</code>算法的一个示例：</p>
<img title="" src="https://pic2.zhimg.com/80/v2-5d1dd1673419599094bf44dd4b533ba9_1440w.jpg" alt="" data-align="inline">
<p>$$
Gain = max { \
Gain, \
\frac{G_1^2}{H_1+\lambda} + \frac{G_{23}^2}{H_{23}+\lambda} - \frac{G_{123}^2}{H_{123}+\lambda} - \lambda, \
\frac{G_{12}^2}{H_{12}+\lambda} + \frac{G_{3}^2}{H_{3}+\lambda} - \frac{G_{123}^2}{H_{123}+\lambda} - \lambda
}
$$</p>
<p>根据样本特征进行排序，然后基于分位数进行划分，并统计三个桶内的$G,H$值，最终求解节点划分的增益。</p>
<p><strong><strong>加权分位数缩略图</strong></strong></p>
<p>事实上， XGBoost 不是简单地按照样本个数进行分位，而是以二阶导数值 $h_i$作为样本的权重进行划分，如下：</p>
<p><img src="https://pic4.zhimg.com/80/v2-5f16246289eaa2a3ae72f971db198457_1440w.jpg" alt=""></p>
<p>为什么要用$h_i$进行样本加权？这是因为通过对$Obj$函数进行变形，可以将$h_i$提取到外部看成是权重。举一个例子：</p>
<p>假设我们分配到一个叶子节点只有一个样本，那么权重最优值计算公式如下：</p>
<p>$$
w^\star_j = (\frac{1}{h_j+\lambda}) (-g_j)
$$</p>
<p>其中$-g_j$就是反向梯度，而$1/(h_j+\lambda)$就是一个学习率。权重的最佳值就是负的梯度乘以一个权重系数，该系数类似于随机梯度下降中的学习率。观察这个权重系数，我们发现，h_j越大，这个系数越小，也就是学习率越小。h_j越大代表什么意思呢？代表在该点附近梯度变化非常剧烈，可能只要一点点的改变，梯度就从10000变到了1，所以，此时，我们在使用反向梯度更新时步子就要小而又小，也就是权重系数要更小。这里是一个直观的解释，完整的公式推导如下：</p>
<p>$$
\begin{equation*}
\begin{split}
Ojb^{(t)} &amp;\approx \sum_{i=1}^n \left[g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right] + \sum_{j=1}^t\Omega(f_i) \
&amp;= \sum_{i=1}^n\left[g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) + \frac{1}{2} \frac{g_i^2}{h_i} \right] + \Omega(f_t) + C \
&amp;= \sum_{i=1}^2\frac{1}{2}h_i \left[f_t(x_i)-(-\frac{g_i}{h_i}) \right]^2 + \Omega(f_t) + C</p>
<p>\end{split}
\end{equation*}
$$</p>
<p>其中$\frac{1}{2}\frac{g_i^2}{h_i}$与$C$都是常数，同时我们可以看到$h_i$就是平方损失函数中样本的权重。</p>
<h2 id="lightgbm"><a href="#lightgbm" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>LightGBM</h2>
<blockquote>
<p>先给一个概念性的总结来给LightGBM vs XGBoost定性：</p>
<ul>
<li>
<p>Hitogram</p>
</li>
<li>
<p>GOSS算法</p>
</li>
<li>
<p>EFB算法</p>
</li>
<li>
<p>Voting算法</p>
</li>
<li>
<p>Leaf-wise树生长</p>
</li>
<li>
<p>Categorical feature内部自动处理</p>
</li>
</ul>
<p>通过上述三个主要算法优化策略，LightGBM对比XGBoost最大的优点是 <strong>快</strong></p>
</blockquote>
<p>xgboost最大的问题是计算效率，从上面对xgboost的介绍也可以看到最大的计算资源消耗在分裂点的计算过程。分裂点计算涉及到：① 对每个feature在所有的样本上的排序，如果样本特征空间很大，以及数据集也很大，那么这个排序需要耗费大量的资源：$N_f \times Ns$。其中$N_f$是特征空间维度，$N_s$是样本数量。LightGBM的提出也正是针对xgboost存在的问题。先给出LightGBM的改进点：</p>
<ul>
<li>
<p>基于Histogram的决策树算法（xgboost也实现了）</p>
</li>
<li>
<p><strong>单边梯度采样（Gradient-based One-Side Sampling, GOSS）</strong> 使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销</p>
</li>
<li>
<p><strong>互斥特征捆绑 (Exclusive Feature Bundling, EFB)</strong> 使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的</p>
</li>
<li>
<p><strong>Leaf-wise决策树生长策略</strong> 大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法</p>
</li>
<li>
<p><strong>直接支持类别特征</strong></p>
</li>
</ul>
<h3 id="直方图策略"><a href="#直方图策略" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>直方图策略</h3>
<p>Histogram algorithm并不是一个很新颖的创新，在统计学被大量使用。这里也只是给一个简单的介绍。直方图算法的基本思想是：先把连续的浮点特征值离散化成 $k$个整数，同时构造一个宽度为 $k$ 的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。特征离散化有很多优点：存储方便、运算更快、鲁棒性强、模型更加稳定等，我们在这里突出最显著的两个优点：</p>
<p><img src="https://www.researchgate.net/publication/346577317/figure/fig3/AS:1001743705993216@1615845719667/Histogram-algorithm.png" alt=""></p>
<p><img src="https://www.researchgate.net/publication/350848994/figure/fig2/AS:1019742676582401@1620137008154/Histogram-algorithm-of-LightGBM.ppm" alt=""></p>
<ul>
<li>    <strong>内存占用小</strong>： 直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用$8$位整型存储就足够了，内存消耗可以降低为原来的 $8$ 。也就是说XGBoost需要用$32$位的浮点数去存储特征值，并用$32$位的整形去存储索引，而 LightGBM只需要用$8$ 位去存储直方图，内存相当于减少为 $\frac{1}{8}$；</li>
</ul>
<p><img src="https://pic4.zhimg.com/v2-3064f201bc8545f851c7ccf47921c0e7_r.jpg" alt=""></p>
<ul>
<li><strong>计算代价更小</strong>：预排序算法XGBoost每遍历一个特征值就需要计算一次分裂的增益，而直方图算法LightGBM只需要计算 $k$次（ $k$可以认为是常数），直接将时间复杂度从$O(#data \times #feature) $ 降低到$ O(k \times #feature) $，而我们知道$#data &gt;&gt; k$。</li>
</ul>
<p>由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但是试验显示离散化的分割点对最终的精确度影响微乎其微，甚至有时候效果更加好。其原因是决策树在这里是一个弱模型（weak learner），分割点不精准对最终模型的影响不大，而且更加粗略的分割点也能一定程度上起到正则化的效果，可以有效防止过拟合。另外，即使单棵决策树的效果不好，但是在梯度提升(Gradient Boosting)的框架下影响不大，这一切都是采用Histogram策略的基础。</p>
<p>LightGBM的另外一个优化是对Histogram做<strong>差加速</strong>。<code>一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍</code>。通常构造直方图时，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。在实际构建树的过程中，LightGBM还可以先计算直方图小的叶子节点，然后利用直方图做差来获得直方图大的叶子节点，这样就可以用非常微小的代价得到它兄弟叶子的直方图。</p>
<p><img src="https://pic4.zhimg.com/v2-b51f2764c13ca0a7b4cb41849a367a87_b.jpg" alt=""></p>
<h3 id="带深度限制的leaf-wise算法"><a href="#带深度限制的leaf-wise算法" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>带深度限制的Leaf-Wise算法</h3>
<p>$$
L(y,\hat{y}) = ln(1+e^{-y\hat{y}}), y \in {-1, 1}) \
g = L^{'} = \frac {-y} {1+e^{y\hat{y}}} \
h = g^{'} = \left(  \frac{-y}{1+e^{y\hat{y}}}\right)^{`} =
$$</p>
<h2 id="catboost"><a href="#catboost" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>CatBoost</h2>

            </div>

            
    
    
        <ul class="post-copyright">
            <li class="copyright-item author"><span class="copyright-item-text">作者</span>：<a href="https://huangpeng1126.github.com/" class="p-author h-card" target="_blank" rel="noopener">瞌睡鱼</a></li>
            
                
                
                
                
                <li class="copyright-item link"><span class="copyright-item-text">链接</span>：<a href="/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/" target="_blank" rel="noopener">https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost分析/</a></li>
            
            <li class="copyright-item license"><span class="copyright-item-text">许可</span>：<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a></li>
            
        </ul>
    



        </article>

        

        
    <div class="updated-badge-container">
        <span title="Updated @ 2022-08-19 14:59:48 CST" style="cursor:help">

<svg xmlns="http://www.w3.org/2000/svg" width="130" height="20" class="updated-badge"><linearGradient id="b" x2="0" y2="100%"><stop offset="0" stop-color="#bbb" stop-opacity=".1"/><stop offset="1" stop-opacity=".1"/></linearGradient><clipPath id="a"><rect width="130" height="20" rx="3" fill="#fff"/></clipPath><g clip-path="url(#a)"><path class="updated-badge-left" d="M0 0h55v20H0z"/><path class="updated-badge-right" d="M55 0h75v20H55z"/><path fill="url(#b)" d="M0 0h130v20H0z"/></g><g fill="#fff" text-anchor="middle" font-size="110"><text x="285" y="150" fill="#010101" fill-opacity=".3" textLength="450" transform="scale(.1)">updated</text><text x="285" y="140" textLength="450" transform="scale(.1)">updated</text><text x="915" y="150" fill="#010101" fill-opacity=".3" textLength="650" transform="scale(.1)">2022-08-19</text><text x="915" y="140" textLength="650" transform="scale(.1)">2022-08-19</text></g></svg>
        </span></div>



        


        <div class="post-share">

        

        <div class="share-items">

            
                <div class="share-item twitter">
                    
                    <a href="https://twitter.com/share?url=https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/&amp;text=Xgboost_lightgbm_catboost%e5%88%86%e6%9e%90&amp;hashtags=&amp;via=reuixiy" title="分享到「Twitter」" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon twitter-icon"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></a>
                </div>
            

            
                <div class="share-item facebook">
                    
                    <a href="https://www.facebook.com/sharer/sharer.php?u=https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/&amp;hashtag=%23" title="分享到「Facebook」" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon facebook-icon"><path d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg></a>
                </div>
            

            
                <div class="share-item linkedin">
                    
                    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/&amp;title=Xgboost_lightgbm_catboost%e5%88%86%e6%9e%90&amp;summary=xgboost%20%e6%a8%a1%e5%9e%8b%e5%ae%9a%e4%b9%89%20%e4%bb%80%e4%b9%88%e6%98%afXGBoost%ef%bc%9a[%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3]%28Introduction%20to%20Boosted%20Trees%20%e2%80%94%20xgboost%202.0.0-dev%20documentation%29%e2%80%a6%e2%80%a6&amp;source=%e7%9e%8c%e7%9d%a1%e9%b1%bc&amp;%e8%8a%b1%e8%84%b8%e7%8c%ab%20%e4%b8%aa%e4%ba%ba%e7%bd%91%e7%ab%99" title="分享到「LinkedIn」" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon linkedin-icon"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a>
                </div>
            

            
                <div class="share-item telegram">
                    
                    <a href="https://t.me/share/url?url=https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/&amp;text=Xgboost_lightgbm_catboost%e5%88%86%e6%9e%90" title="分享到「Telegram」" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon telegram-icon"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm121.8 169.9l-40.7 191.8c-3 13.6-11.1 16.9-22.4 10.5l-62-45.7-29.9 28.8c-3.3 3.3-6.1 6.1-12.5 6.1l4.4-63.1 114.9-103.8c5-4.4-1.1-6.9-7.7-2.5l-142 89.4-61.2-19.1c-13.3-4.2-13.6-13.3 2.8-19.7l239.1-92.2c11.1-4 20.8 2.7 17.2 19.5z"/></svg></a>
                </div>
            

            
                <div class="share-item weibo">
                    
                    <a href="https://service.weibo.com/share/share.php?&amp;url=https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/&amp;title=Xgboost_lightgbm_catboost%e5%88%86%e6%9e%90&amp;pic=https://pic2.zhimg.com/80/v2-c108a582cd03dae298c5c51305498cf5_1440w.jpg&amp;searchPic=false" title="分享到「新浪微博」" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon weibo-icon"><path d="M407 177.6c7.6-24-13.4-46.8-37.4-41.7-22 4.8-28.8-28.1-7.1-32.8 50.1-10.9 92.3 37.1 76.5 84.8-6.8 21.2-38.8 10.8-32-10.3zM214.8 446.7C108.5 446.7 0 395.3 0 310.4c0-44.3 28-95.4 76.3-143.7C176 67 279.5 65.8 249.9 161c-4 13.1 12.3 5.7 12.3 6 79.5-33.6 140.5-16.8 114 51.4-3.7 9.4 1.1 10.9 8.3 13.1 135.7 42.3 34.8 215.2-169.7 215.2zm143.7-146.3c-5.4-55.7-78.5-94-163.4-85.7-84.8 8.6-148.8 60.3-143.4 116s78.5 94 163.4 85.7c84.8-8.6 148.8-60.3 143.4-116zM347.9 35.1c-25.9 5.6-16.8 43.7 8.3 38.3 72.3-15.2 134.8 52.8 111.7 124-7.4 24.2 29.1 37 37.4 12 31.9-99.8-55.1-195.9-157.4-174.3zm-78.5 311c-17.1 38.8-66.8 60-109.1 46.3-40.8-13.1-58-53.4-40.3-89.7 17.7-35.4 63.1-55.4 103.4-45.1 42 10.8 63.1 50.2 46 88.5zm-86.3-30c-12.9-5.4-30 .3-38 12.9-8.3 12.9-4.3 28 8.6 34 13.1 6 30.8.3 39.1-12.9 8-13.1 3.7-28.3-9.7-34zm32.6-13.4c-5.1-1.7-11.4.6-14.3 5.4-2.9 5.1-1.4 10.6 3.7 12.9 5.1 2 11.7-.3 14.6-5.4 2.8-5.2 1.1-10.9-4-12.9z"/></svg></a>
                </div>
            

            
                <div class="share-item douban">
                    
                    <a href="https://www.douban.com/share/service?href=https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/&amp;name=Xgboost_lightgbm_catboost%e5%88%86%e6%9e%90&amp;text=xgboost%20%e6%a8%a1%e5%9e%8b%e5%ae%9a%e4%b9%89%20%e4%bb%80%e4%b9%88%e6%98%afXGBoost%ef%bc%9a[%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3]%28Introduction%20to%20Boosted%20Trees%20%e2%80%94%20xgboost%202.0.0-dev%20documentation%29%e2%80%a6%e2%80%a6" title="分享到「豆瓣」" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon douban-icon"><path d="M.643.92v2.412h22.714V.92H.643zm1.974 4.926v9.42h18.764v-9.42H2.617zm2.72 2.408H18.69v4.605H5.338V8.254zm1.657 7.412l-2.512.938c1.037 1.461 1.87 2.825 2.512 4.091H0v2.385h24v-2.385h-6.678c.818-1.176 1.589-2.543 2.303-4.091l-2.73-.938a29.952 29.952 0 01-2.479 5.03h-4.75c-.786-1.962-1.677-3.641-2.672-5.03Z"/></svg></a>
                </div>
            

            
                <div class="share-item qq">
                    
                    <a href="https://connect.qq.com/widget/shareqq/index.html?url=https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/&amp;title=Xgboost_lightgbm_catboost%e5%88%86%e6%9e%90&amp;summary=xgboost%20%e6%a8%a1%e5%9e%8b%e5%ae%9a%e4%b9%89%20%e4%bb%80%e4%b9%88%e6%98%afXGBoost%ef%bc%9a[%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3]%28Introduction%20to%20Boosted%20Trees%20%e2%80%94%20xgboost%202.0.0-dev%20documentation%29%e2%80%a6%e2%80%a6&amp;pics=https://pic2.zhimg.com/80/v2-c108a582cd03dae298c5c51305498cf5_1440w.jpg&amp;site=%e7%9e%8c%e7%9d%a1%e9%b1%bc&amp;%e8%8a%b1%e8%84%b8%e7%8c%ab%20%e4%b8%aa%e4%ba%ba%e7%bd%91%e7%ab%99" title="分享到「QQ」" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon qq-icon"><path d="M433.754 420.445c-11.526 1.393-44.86-52.741-44.86-52.741 0 31.345-16.136 72.247-51.051 101.786 16.842 5.192 54.843 19.167 45.803 34.421-7.316 12.343-125.51 7.881-159.632 4.037-34.122 3.844-152.316 8.306-159.632-4.037-9.045-15.25 28.918-29.214 45.783-34.415-34.92-29.539-51.059-70.445-51.059-101.792 0 0-33.334 54.134-44.859 52.741-5.37-.65-12.424-29.644 9.347-99.704 10.261-33.024 21.995-60.478 40.144-105.779C60.683 98.063 108.982.006 224 0c113.737.006 163.156 96.133 160.264 214.963 18.118 45.223 29.912 72.85 40.144 105.778 21.768 70.06 14.716 99.053 9.346 99.704z"/></svg></a>
                </div>
            

            
                <div class="share-item qzone">
                    
                    <a href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://huangpeng1126.github.io/posts/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90/&amp;title=Xgboost_lightgbm_catboost%e5%88%86%e6%9e%90&amp;summary=xgboost%20%e6%a8%a1%e5%9e%8b%e5%ae%9a%e4%b9%89%20%e4%bb%80%e4%b9%88%e6%98%afXGBoost%ef%bc%9a[%e5%ae%98%e6%96%b9%e6%96%87%e6%a1%a3]%28Introduction%20to%20Boosted%20Trees%20%e2%80%94%20xgboost%202.0.0-dev%20documentation%29%e2%80%a6%e2%80%a6&amp;pics=https://pic2.zhimg.com/80/v2-c108a582cd03dae298c5c51305498cf5_1440w.jpg&amp;site=%e7%9e%8c%e7%9d%a1%e9%b1%bc&amp;%e8%8a%b1%e8%84%b8%e7%8c%ab%20%e4%b8%aa%e4%ba%ba%e7%bd%91%e7%ab%99" title="分享到「QQ 空间」" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon qzone-icon"><path d="M23.985 9.202c-.032-.099-.127-.223-.334-.258-.207-.036-7.351-1.406-7.351-1.406s-.105-.022-.198-.07c-.092-.047-.127-.167-.127-.167S12.447.956 12.349.77C12.25.583 12.104.532 12 .532c-.104 0-.251.051-.349.238-.098.186-3.626 6.531-3.626 6.531s-.035.12-.128.167c-.092.047-.197.07-.197.07S.556 8.908.348 8.943c-.208.036-.302.16-.333.258a.477.477 0 0 0 .125.449l5.362 5.49s.072.08.119.172c.016.104.005.21.005.21s-1.189 7.242-1.22 7.45.075.369.159.43c.083.062.233.106.421.013.189-.093 6.812-3.261 6.812-3.261s.098-.044.201-.061c.103-.017.201.061.201.061s6.623 3.168 6.812 3.261c.188.094.338.049.421-.013a.463.463 0 0 0 .159-.43c-.021-.14-.93-5.677-.93-5.677.876-.54 1.425-1.039 1.849-1.747-2.594.969-6.006 1.717-9.415 1.866-.915.041-2.41.097-3.473-.015-.678-.071-1.17-.144-1.243-.438-.053-.215.054-.46.545-.831a2640.5 2640.5 0 0 1 2.861-2.155c1.285-.968 3.559-2.47 3.559-2.731 0-.285-2.144-.781-4.037-.781-1.945 0-2.275.132-2.811.168-.488.034-.769.005-.804-.138-.06-.248.183-.389.588-.568.709-.314 1.86-.594 1.984-.626.194-.052 3.082-.805 5.618-.535 1.318.14 3.244.668 3.244 1.276 0 .342-1.721 1.494-3.225 2.597-1.149.843-2.217 1.561-2.217 1.688 0 .342 3.533 1.241 6.689 1.01l.003-.022c.048-.092.119-.172.119-.172l5.362-5.49a.477.477 0 0 0 .127-.449z"/></svg></a>
                </div>
            

            
                <div class="share-item qrcode">
                    <div class="qrcode-container" title="通过「二维码」"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon qrcode-icon"><path d="M0 224h192V32H0v192zM64 96h64v64H64V96zm192-64v192h192V32H256zm128 128h-64V96h64v64zM0 480h192V288H0v192zm64-128h64v64H64v-64zm352-64h32v128h-96v-32h-32v96h-64V288h96v32h64v-32zm0 160h32v32h-32v-32zm-64 0h32v32h-32v-32z"/></svg><div id="qrcode-img"></div>
                    </div>
                    <script src="https://cdn.jsdelivr.net/npm/qrcode-generator@1.4.4/qrcode.min.js"></script>

<script>
    var typeNumber = 0;
    var errorCorrectionLevel = 'L';
    var qr = qrcode(typeNumber, errorCorrectionLevel);
    qr.addData('https:\/\/huangpeng1126.github.io\/posts\/xgboost_lightgbm_catboost%E5%88%86%E6%9E%90\/');
    qr.make();
    document.getElementById('qrcode-img').innerHTML = qr.createImgTag();
</script>

                </div>
            

        </div>

    </div>




        
    
    



        
    



        


        


        
    
        
        
    
    
    
    
        <ul class="post-nav">
            
                <li class="post-nav-prev">
                    <a href="/posts/%E5%B8%B8%E8%A7%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/" rel="prev">&lt; 常见损失函数介绍</a>
                </li>
            
            
                <li class="post-nav-next">
                    <a href="/posts/lucene%E5%90%8C%E4%B9%89%E8%AF%8D%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/" rel="next">Lucene同义词实现分析 &gt;</a>
                </li>
            
        </ul>
    



        


    </div>
</main>


            
    <div id="back-to-top" class="back-to-top">
        <a href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a>
    </div>


            
    <footer id="footer" class="footer">
        <div class="footer-inner">
            <div class="site-info">©&nbsp;1969–2022&nbsp;<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon footer-icon"><path d="M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3 0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z"/></svg>&nbsp;瞌睡鱼</div><div class="powered-by">Powered by <a href="https://github.com/gohugoio/hugo" target="_blank" rel="noopener">Hugo</a> | Theme is <a href="https://github.com/reuixiy/hugo-theme-meme" target="_blank" rel="noopener">MemE</a></div><div class="site-copyright"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a></div>

            
    
        <ul class="socials"><li class="socials-item">
                    <a href="/rss.xml" target="_blank" rel="external noopener" title="RSS"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M19.199 24C19.199 13.467 10.533 4.8 0 4.8V0c13.165 0 24 10.835 24 24h-4.801zM3.291 17.415c1.814 0 3.293 1.479 3.293 3.295 0 1.813-1.485 3.29-3.301 3.29C1.47 24 0 22.526 0 20.71s1.475-3.294 3.291-3.295zM15.909 24h-4.665c0-6.169-5.075-11.245-11.244-11.245V8.09c8.727 0 15.909 7.184 15.909 15.91z"/></svg></a>
                </li><li class="socials-item">
                    <a href="mailto:reuixiy@gmail.com" target="_blank" rel="external noopener" title="Email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon social-icon"><path d="M464 64H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm0 48v40.805c-22.422 18.259-58.168 46.651-134.587 106.49-16.841 13.247-50.201 45.072-73.413 44.701-23.208.375-56.579-31.459-73.413-44.701C106.18 199.465 70.425 171.067 48 152.805V112h416zM48 400V214.398c22.914 18.251 55.409 43.862 104.938 82.646 21.857 17.205 60.134 55.186 103.062 54.955 42.717.231 80.509-37.199 103.053-54.947 49.528-38.783 82.032-64.401 104.947-82.653V400H48z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://github.com/reuixiy" target="_blank" rel="external noopener" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://twitter.com/reuixiy" target="_blank" rel="external noopener" title="Twitter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon social-icon"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://t.me/yixiuer" target="_blank" rel="external noopener" title="Telegram"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon social-icon"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm121.8 169.9l-40.7 191.8c-3 13.6-11.1 16.9-22.4 10.5l-62-45.7-29.9 28.8c-3.3 3.3-6.1 6.1-12.5 6.1l4.4-63.1 114.9-103.8c5-4.4-1.1-6.9-7.7-2.5l-142 89.4-61.2-19.1c-13.3-4.2-13.6-13.3 2.8-19.7l239.1-92.2c11.1-4 20.8 2.7 17.2 19.5z"/></svg></a>
                </li></ul>
    



            
        </div>
    </footer>


        </div>
        

        
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha256-gPJfuwTULrEAAcI3X4bALVU/2qBU+QY/TpoD3GO+Exw=" crossorigin="anonymous">
<script>
    if (typeof renderMathInElement === 'undefined') {
        var getScript = (options) => {
            var script = document.createElement('script');
            script.defer = true;
            script.crossOrigin = 'anonymous';
            Object.keys(options).forEach((key) => {
                script[key] = options[key];
            });
            document.body.appendChild(script);
        };
        getScript({
            src: 'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js',
            integrity: 'sha256-YTW9cMncW/ZQMhY69KaUxIa2cPTxV87Uh627Gf5ODUw=',
            onload: () => {
                getScript({
                    src: 'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/mhchem.min.js',
                    integrity: 'sha256-yzSfYeVsWJ1x+2g8CYHsB/Mn7PcSp8122k5BM4T3Vxw=',
                    onload: () => {
                        getScript({
                            src: 'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js',
                            integrity: 'sha256-fxJzNV6hpc8tgW8tF0zVobKa71eTCRGTgxFXt1ZpJNM=',
                            onload: () => {
                                renderKaTex();
                            }
                        });
                    }
                });
            }
        });
    } else {
        renderKaTex();
    }
    function renderKaTex() {
        renderMathInElement(
            document.body,
            {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false}
                ]
            }
        );
    }
</script>




    <script>
    if (typeof MathJax === 'undefined') {
        window.MathJax = {
            loader: {
                load: ['[tex]/mhchem']
            },
            
            tex: {
                inlineMath: {'[+]': [['$', '$']]},
                tags: 'ams',
                packages: {'[+]': ['mhchem']}
            }
        };
        (function() {
            var script = document.createElement('script');
            script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
            script.defer = true;
            document.head.appendChild(script);
        })();
    } else {
        MathJax.texReset();
        MathJax.typeset();
    }
</script>




    <script src="https://cdn.jsdelivr.net/npm/mermaid@8.8.3/dist/mermaid.min.js"></script>
<script>
    let mermaidConfig = {
        startOnLoad: true,
        flowchart: {
            useMaxWidth: false,
            htmlLabels: true
        },
        theme: 'default'
    };
    mermaid.initialize(mermaidConfig);
</script>





    <script src="https://cdn.jsdelivr.net/npm/medium-zoom@latest/dist/medium-zoom.min.js"></script>

<script>
    let imgNodes = document.querySelectorAll('div.post-body img');
    imgNodes = Array.from(imgNodes).filter(node => node.parentNode.tagName !== "A");

    mediumZoom(imgNodes, {
        background: 'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'
    })
</script>




    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module" defer></script>







    </body>
</html>
